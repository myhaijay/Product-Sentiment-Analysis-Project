{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50809abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize columns\n",
    "standard_columns = [data.columns[i].lower() for i in range(len(data.columns))]\n",
    "standard_columns = [col_name.lower().replace(' ', '_') for col_name in data.columns]\n",
    "data.columns = standard_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95930b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers automatically\n",
    "def remove_outliers(df, threshold=1.5, in_columns=df.select_dtypes(np.number).columns, skip_columns=[]):\n",
    "    for column in in_columns:\n",
    "        if column not in skip_columns:\n",
    "            upper = np.percentile(df[column],75)\n",
    "            lower = np.percentile(df[column],25)\n",
    "            iqr = upper - lower\n",
    "            upper_limit = upper + threshold * iqr\n",
    "            lower_limit = lower - threshold * iqr\n",
    "            df = df[(df[column]>lower_limit) & (df[column]<upper_limit)]\n",
    "    return df\n",
    "#df2_ = remove_outliers(df2_) - apply\n",
    "#df2 = remove_outliers(df1, threshold=1.5, in_columns=['target_d', 'avggift']) - remove outliers from specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29265ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing weight to kilograms(float)\n",
    "def weight_kilos(x): \n",
    "    weight = str(x).replace(\"lbs\",' ')\n",
    "    new_weight = round(float(weight)*0.4535,0) \n",
    "    return new_weight\n",
    "#data['weight']  = data['weight'].apply(lambda x: weight_kilos(x)) - Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing height to centimeters(float)\n",
    "def height_centimeters(x): \n",
    "    height = str(x).replace(\"'\",' ').replace('\"',' ').split()\n",
    "    new_height = float(height[0])*30.48 + float(height[1])*2.54\n",
    "    return new_height\n",
    "#my_data2['height']  = my_data2['height'].apply(lambda x: height_centimeters(x)) - Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of nulls in a datafroma\n",
    "nulls = pd.DataFrame(data.isna().sum()*100/len(data), columns=['percentage'])\n",
    "nulls.sort_values('percentage', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e414d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe:  Check those that gave\n",
    "len(df[(df['target_d'] > 10) & (df['target_d'] < 50)]) , len(df[df['target_d'] >= 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deaing with categorical columns\n",
    "# Remove those with more than 50 different categories\n",
    "remove_cols = []\n",
    "\n",
    "for col in dfcat:\n",
    "    if len(dfcat[col].unique()) > 50:\n",
    "        display(dfcat[col].value_counts())\n",
    "        remove_cols.append(col)\n",
    "        \n",
    "len(remove_cols)\n",
    "# if we had more time, analysing one by one and checking the possibility of bucketing them would be ideal\n",
    "# another important thing would be to visualize the average donations within each category, to check for patterns\n",
    "# yet another approach would be to check for columns with hierarchical values,\n",
    "# so you could replace by numerical and discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first run, I prefer not to have any scaling, just to have a benchmark on my metrics\n",
    "# then i come back to these steps and check different scaling techniques to see which is better\n",
    "\n",
    "# usually you'll choose one scaling technique for the whole dataframe\n",
    "\n",
    "# if you are going for StandardScaler, MinMaxScaler or Normalizer, remember to do it after the splits:\n",
    "## X-y split for Normalizer;\n",
    "## train-test split for the other two\n",
    "\n",
    "# for this example i chose BoxCox transformation\n",
    "\n",
    "def boxcox_transform(df):\n",
    "    numeric_cols = df.select_dtypes(np.number).columns\n",
    "    _ci = {column: None for column in numeric_cols}\n",
    "    for column in numeric_cols:\n",
    "        # since i know any columns should take negative numbers, to avoid -inf in df\n",
    "        df[column] = np.where(df[column]<=0, np.NAN, df[column]) \n",
    "        df[column] = df[column].fillna(df[column].mean())\n",
    "        transformed_data, ci = stats.boxcox(df[column])\n",
    "        df[column] = transformed_data\n",
    "        _ci[column] = [ci] \n",
    "    return df, _ci\n",
    "\n",
    "# df, _ci = boxcox_transform(df) - Apply - if you want to overwrite the already existing df\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap\n",
    "corr_matrix=data.corr(method='pearson')  # default\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot distribution of numerical variables\n",
    "for col in data.select_dtypes(np.number):\n",
    "    sns.displot(data[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation since it will make the 'outliers' interval smaller\n",
    "def log_transform(x):\n",
    "    if np.isfinite(x) and x!=0:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "# or\n",
    "log_transform = lambda x: np.log(x) if np.isfinite(x) and x!=0 else np.NAN\n",
    "\n",
    "\n",
    "def log_scaled(df):\n",
    "    numeric_cols = df.select_dtypes(np.number).columns\n",
    "    for column in numeric_cols:\n",
    "        df[column] = df[column].apply(log_transform).fillna(df[column].mean()) \n",
    "    return df\n",
    "# data1 = data.copy()\n",
    "\n",
    "# data1['TIMELAG'] = data1['TIMELAG'].apply(log_transform).fillna(data1['TIMELAG'].mean()) - Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b207d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NaNs with the mean of the columns\n",
    "def fill_nan_with_mean(df):\n",
    "    numeric_cols = df.select_dtypes(np.number).columns\n",
    "    for column in numeric_cols:\n",
    "        if df[column].isna().sum() != 0:\n",
    "            df[column] = df[column].fillna(df[column].mean()) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# apply log10 transformation since it will make the 'outliers' interval smaller\n",
    "def log_transform2(x):\n",
    "    if np.isfinite(x) and x!=0:\n",
    "        return math.log10(x)\n",
    "    else:\n",
    "        return np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering scaled dataframe to get a \"new\" dataframe\n",
    "X_scaled = X.applymap(lambda x: np.nan if x == 0 else x).applymap(np.log).fillna(0)\n",
    "y_scaled = y.apply(lambda x: np.nan if x == 0 else x).apply(np.log).fillna(0)\n",
    "\n",
    "y_scaled_higher = y_scaled[y_scaled > 0]\n",
    "x_scaled_higher = X_scaled.loc[y_scaled_higher.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4055b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler to scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = StandardScaler().fit(numerics)\n",
    "scaled = transformer.transform(numerics)\n",
    "scaled = pd.DataFrame(scaled)\n",
    "scaled.columns = numerics.columns \n",
    "scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_multiclass(X,y): #X,y are pandas df and series\n",
    "    y=y.astype(str)   #convert to string to onehot encode\n",
    "    enc=ce.OneHotEncoder().fit(y)\n",
    "    y_onehot=enc.transform(y)\n",
    "    class_names=y_onehot.columns  #names of onehot encoded columns\n",
    "    X_obj=X.select_dtypes('object') #separate categorical columns\n",
    "    X=X.select_dtypes(exclude='object') \n",
    "    for class_ in class_names:\n",
    "            \n",
    "        enc=ce.TargetEncoder()\n",
    "        enc.fit(X_obj,y_onehot[class_]) #convert all categorical \n",
    "        temp=enc.transform(X_obj)       #columns for class_\n",
    "        temp.columns=[str(x)+'_'+str(class_) for x in temp.columns]\n",
    "        X=pd.concat([X,temp],axis=1)    #add to original dataset\n",
    "      \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "pca = PCA()\n",
    "pca.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ec52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use RandomForest - Classification problem\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_group, test_size=0.3, random_state=42)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_group_preds = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_group_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you need to use the encode somewhere else besides your notebook:\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first')\n",
    "encoder.fit(X_cat)\n",
    "\n",
    "encoded = encoder.transform(X_cat).toarray()\n",
    "encoded #.shape # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_num, encoded], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99afdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00505b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions using X_test data\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d680f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get your linear metrics using y_test data - r-squared, mae, rmse\n",
    "r2_score(y_test, predictions), mean_absolute_error(y_test, predictions), mean_squared_error(y_test, predictions, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Reverse Boxcox scaling\n",
    "# MAE and MSE are log transformed\n",
    "# have to inverse transform\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "predictions = inv_boxcox(predictions, _ci['target_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dealing with Ordinal Categoricals - Hard coding \n",
    "customer_cats[\"coverage\"] = customer_cats[\"coverage\"].map({\"Basic\" : 0, \"Extended\" : 1, \"Premium\" : 2})\n",
    "customer_cats[\"education\"] = customer_cats[\"education\"].map({\"High School or Below\" : 0, \"College\" : 1, \"Bachelor\" : 2,\n",
    "                                                                          \"Master\" : 3, \"Doctor\": 4})\n",
    "customer_cats[\"employmentstatus\"] = customer_cats[\"employmentstatus\"].map({\"Unemployed\" : 0, \"Disabled\" : 1, \"Retired\" : 2,\n",
    "                                                                          \"Medical Leave\" : 3, \"Employed\": 4})\n",
    "customer_cats[\"location_code\"] = customer_cats[\"location_code\"].map({\"Rural\" : 0, \"Suburban\" : 1, \"Urban\" : 2})\n",
    "customer_cats[\"vehicle_size\"] = customer_cats[\"vehicle_size\"].map({\"Small\" : 0, \"Medsize\" : 1, \"Large\" : 2})\n",
    "customer_cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc14a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot categorical variable\n",
    "sns.catplot(x=\"zipcode\", y=\"price\", data=df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58203905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text formatting styles\n",
    "print(f'''This model saved ${savings} in mails.\n",
    "        Missed ${total_missed_donations} in donations and wasted ${total_extra_spent_on_mail} in mails not responded.\n",
    "        You made: ${sum(df['target_d']) - total_amount_spent_on_sent_mails}\n",
    "           ''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the variance threshold technique\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selection = VarianceThreshold(threshold=(.9))\n",
    "# This drops the columns that have a variance less than this threshold\n",
    "selection.fit(numerical)\n",
    "temp = selection.transform(numerical)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45446df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlations between variables with heatmaps\n",
    "corr = dff_estate.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20))\n",
    "\n",
    "ax = sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, center=0,\n",
    "            square=True, linewidths=.5,annot=True, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Upsampling\n",
    "# getting sample with the same amount as the majority class\n",
    "A = data[data['status'] == 'A'].sample(400, replace=True) # needs the replace because it has less than 400 rows\n",
    "B = data[data['status'] == 'B'].sample(400, replace=True)\n",
    "C = data[data['status'] == 'C'].sample(400) # don't need the replace because it has 403 rows\n",
    "D = data[data['status'] == 'D'].sample(400, replace=True)\n",
    "\n",
    "upsampled = pd.concat([A, B, C, D]).sample(frac=1) # .sample(frac=1) here is just to shuffle the dataframe\n",
    "upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsampling automatically using SMOTE function\n",
    "# SMOTE\n",
    "# Uses knn to create rows with similar features from the minority classes.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "X = data.drop('status', axis=1)\n",
    "y = data['status']\n",
    "\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually downsampling\n",
    "# getting sample with the same amount as the minority class\n",
    "A = data[data['status'] == 'A'].sample(30)\n",
    "B = data[data['status'] == 'B'].sample(30)\n",
    "C = data[data['status'] == 'C'].sample(30)\n",
    "D = data[data['status'] == 'D'].sample(30)\n",
    "\n",
    "downsampled = pd.concat([A, B, C, D]).sample(frac=1) # .sample(frac=1) here is just to shuffle the dataframe\n",
    "downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704463d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomek Links\n",
    "# Pairs of almost similar rows from opposite classes.\n",
    "# Removing the row of the majority class from each pair helps the classifier.\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "X = data.drop('status', axis=1)\n",
    "y = data['status']\n",
    "\n",
    "tomek = TomekLinks()\n",
    "X_tl, y_tl = tomek.fit_resample(X, y)\n",
    "y_tl.value_counts()\n",
    "\n",
    "## Ooops - good for smaller imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43168c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Evaluating Logistic regression model results\n",
    "cf_matrix = confusion_matrix(y_test, predictions)\n",
    "group_names = ['True A', 'False A', 'False A', 'False A',\n",
    "               'False B', 'True B', 'False B', 'False B',\n",
    "               'False c', 'False C', 'True C', 'False C',\n",
    "               'False D', 'False D', 'False D', 'True D']\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(4,4)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classification, X_test, y_test, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd467e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To merge dataframes \n",
    "general_model = model_5.merge(results, left_index = True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c133e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# watch out not to inflate your metrics, ideally:\n",
    "# you do the train-test split first and fit_resample only on the training set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.3, random_state=42)\n",
    "\n",
    "tl = TomekLinks('majority')\n",
    "\n",
    "X_tl, y_tl = tl.fit_resample(np.array(X_train), y_train)\n",
    "\n",
    "y_tl.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33329b",
   "metadata": {},
   "source": [
    "### Some miscellenous functions and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data summary using groupby and aggregations\n",
    "warshers.groupby('BrandName')[['Volume']].agg(['mean','median','min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Composite Visualization\n",
    "df.groupby('num_vrb')['cat_vrb'].value_counts() #get unique values of the variable we want\n",
    "\n",
    "df.groupby('num_vrb')['cat_vrb'].value_counts().unstack() #pivot the innermost index(cat_vrb) to column labels\n",
    "#Create the stacked bar chart using the pivot table created above \n",
    "df.groupby('num_vrb')['cat_vrb'].value_counts().unstack().plot(kind='bar',stacked=True,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_customer['effective_to_date'].astype(str).str.split('-').apply(lambda x: x[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df872f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to take in several models and compare their metrics\n",
    "def train_test_models(models, X_train, X_test, y_train, y_test):\n",
    "    scores = {}\n",
    "    model_names = [ ]\n",
    "    for mod in models:\n",
    "        model_names.append(str(mod)[:-2])\n",
    "    \n",
    "\n",
    "    for i, model_ in enumerate(models):\n",
    "        model = model_.fit(X_train, y_train)\n",
    "        model_prediction = model.predict(X_test)\n",
    "        r_squared = r2_score(y_test, model_prediction)\n",
    "        mae = mean_absolute_error(y_test, model_prediction)\n",
    "        msqe = mean_squared_error(y_test, model_prediction, squared=False)\n",
    "        \n",
    "\n",
    "        scores[model_names[i]] = [r_squared, mae, msqe]\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65241f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db516ad9",
   "metadata": {},
   "source": [
    "### Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of multiple paga scrapping\n",
    "#Extract the total number of songs\n",
    "sp.find('div', attrs={'class': \"fts-header__title-container\"}).find('h1').get_text().split()[2]\n",
    "# Convert that number to integer\n",
    "total_songs = int(sp.find('div', attrs={'class': \"fts-header__title-container\"}).find('h1').get_text().split()[2])\n",
    "total_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = range(total_songs,0,-10) # because, 10 items per page and counting from highest(top - down)\n",
    "list(starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get song titles and artists from one webpage\n",
    "Titles = []\n",
    "Artists = []\n",
    "\n",
    "for song in songs_all:\n",
    "    Titles.append(song.find('h3').get_text()) \n",
    "    Artists.append(song.find('span').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42865396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way\n",
    "for a in table:\n",
    "    if a.find('h3'):   \n",
    "        print(a.find('h3').get_text())\n",
    "\n",
    "title = [a.find('h3').get_text().strip('\\n') for a in table if a.find('h3')]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attributes in a container\n",
    "def get_movie_info(movies):    \n",
    "    titles = []\n",
    "    ratings = []\n",
    "    genres = []\n",
    "    runtimes = []\n",
    "    links = []\n",
    "\n",
    "    for movie in movies:\n",
    "        title = movie.find('h3').find('a').get_text() if movie.find('h3').find('a') else 'Not informed.'\n",
    "        rating = movie.find('strong').get_text() if movie.find('strong') else 'Not informed.'\n",
    "        genre = movie.find('span', attrs={'class': 'genre'}).get_text(strip=True) if movie.find('span', attrs={'class': 'genre'}) else 'Not informed.'\n",
    "        runtime = movie.find('span', attrs={'class': 'runtime'}).get_text() if movie.find('span', attrs={'class': 'runtime'}) else 'Not informed.'\n",
    "        link = 'http://www.imdb.com' + movie.find('h3', attrs={'class': 'lister-item-header'}).find('a').get('href')\n",
    "\n",
    "        titles.append(title)\n",
    "        ratings.append(rating)\n",
    "        genres.append(genre)\n",
    "        links.append(link)\n",
    "        runtimes.append(runtime)\n",
    "    \n",
    "    dct = {'title': titles, 'rating': ratings, 'genre': genres, 'runtime': runtimes}\n",
    "    \n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the pages of a website using the attributes from above\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for start in starts:\n",
    "    r = requests.get(f'https://www.imdb.com/search/title/?title_type=feature&release_date=2021-01-01,&user_rating=6.5,&num_votes=100,&start={start}&ref_=adv_nxt')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    movies = soup.find_all('div', attrs={'class': \"lister-item-content\"})\n",
    "    info_dct = get_movie_info(movies) # function from above for getting the attributes\n",
    "    new_df = pd.DataFrame.from_dict(info_dct)\n",
    "    df = pd.concat([df, new_df])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83183b2f",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to import and general settings\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dad7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuations\n",
    "    #tokens = re.split('\\W+', text) or\n",
    "    tokens = re.findall('\\w+', text) #tokenize text\n",
    "    text = [word for word in tokens if word not in stopwords] #remove stopwords\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower())) #application\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming tokenized text\n",
    "def stemming(tokenized_list):\n",
    "    text = [ps.stem(word) for word in tokenized_list]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88110307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing tokenized text\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vectorized tokens to array and then to a dataframe\n",
    "X_count_df = pd.DataFrame(X_count_sample.toarray())\n",
    "#Assign the names of the vector columns to the Dataframe columns\n",
    "X_count_df.columns = count_vect_sample.get_feature_names()\n",
    "X_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete Read & Clean raw text\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7828d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build own Grid-Search\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))\n",
    "#Run it\n",
    "\n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
